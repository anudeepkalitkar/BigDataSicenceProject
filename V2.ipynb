{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask_mail import Mail\n",
    "from flask_restful import Api\n",
    "from apispec import APISpec\n",
    "from flask_cors import CORS\n",
    "from apispec.ext.marshmallow import MarshmallowPlugin\n",
    "from flask_apispec.extension import FlaskApiSpec\n",
    "from flask_restful import Resource, fields\n",
    "from flask_apispec import marshal_with, doc, use_kwargs\n",
    "from marshmallow import Schema, fields\n",
    "from flask_apispec.views import MethodResource\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from flask_cors import  cross_origin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, LSTM, Dense, Flatten\n",
    "from keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    "    TensorBoard,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tensorflow version:\", tf.__version__)\n",
    "physicalDevices = tf.config.list_physical_devices(\"GPU\")\n",
    "print(physicalDevices)\n",
    "\n",
    "if len(physicalDevices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physicalDevices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelDir = \"./Models/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H\") + \"/\"\n",
    "if not os.path.exists(ModelDir):\n",
    "    os.mkdir(ModelDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "modelPath = ModelDir + \"model1.sav\"\n",
    "logsDir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H\")\n",
    "tensorboardCBK = TensorBoard(log_dir=logsDir, histogram_freq=1)\n",
    "earlyStoppingCBK = EarlyStopping(\n",
    "    monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "modelCBK = ModelCheckpoint(\n",
    "    modelPath+'.mcp.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduceLRPlateauCBK = ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.1, patience=7, verbose=1, mode='min')\n",
    "callbacks = [earlyStoppingCBK, \n",
    "             reduceLRPlateauCBK, tensorboardCBK]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from metadata import BICYCLEMETADATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BicycleDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        bicycleFolderPath: str,\n",
    "        metaDataFilepath: str,\n",
    "        columnsRetain: list = [\"day\", \"Total\"],\n",
    "    ):\n",
    "        self.folderPath = bicycleFolderPath\n",
    "        self.columnsRetain = columnsRetain\n",
    "        self.metaData = json.load(open(metaDataFilepath, \"r\"))[\"BICYCLEMETADATA\"]\n",
    "        self.directionsMapping = {\n",
    "            \"NorthBound\": 1,\n",
    "            \"SouthBound\": 2,\n",
    "            \"WestBound\": 3,\n",
    "            \"EastBound\": 4,\n",
    "        }\n",
    "        self.bicycleDataFrame = self.LoadDataSet()\n",
    "\n",
    "    def DropColumns(\n",
    "        self,\n",
    "        dataFrame: pd.DataFrame,\n",
    "        renameColumns: dict,\n",
    "    ):\n",
    "        dataFrame = dataFrame[self.columnsRetain]\n",
    "        dataFrame = dataFrame.rename(columns=renameColumns)\n",
    "        return dataFrame\n",
    "\n",
    "    def ConvertDaytoDateTime(self, dataFrame: pd.DataFrame):\n",
    "        dataFrame[\"day\"] = pd.to_datetime(dataFrame[\"day\"])\n",
    "        dataFrame = dataFrame.sort_values(by=\"day\")\n",
    "        return dataFrame\n",
    "\n",
    "    def Get1HrIntervals(self, dataFrame: pd.DataFrame, columnName: str):\n",
    "        dataFrame = dataFrame.resample(\"1H\", on=columnName).sum().reset_index()\n",
    "        return dataFrame\n",
    "\n",
    "    def ConcatDataFrames(self, dataFrames: list):\n",
    "        dataFrame = pd.concat(dataFrames, axis=1)\n",
    "        retainColumns = ~dataFrame.columns.duplicated()\n",
    "        dataFrame = dataFrame.loc[:, retainColumns]\n",
    "        return dataFrame\n",
    "\n",
    "    def FindBestDirections(self, row: np.ndarray):\n",
    "        maxValue = row.max()\n",
    "        return [\n",
    "            self.directionsMapping[direction]\n",
    "            for direction in row.index\n",
    "            if row[direction] == maxValue\n",
    "        ]\n",
    "\n",
    "    def LoadDataSet(self):\n",
    "        if os.path.isdir(self.folderPath) and self.folderPath[-1] != \"/\":\n",
    "            print(\"enter a valid folderPath\")\n",
    "        else:\n",
    "            bicycleDataFrame = None\n",
    "            for data in self.metaData:\n",
    "                print(\"Reading DataSet from\", data[\"filename\"])\n",
    "                dataFrame = pd.read_csv(\n",
    "                    self.folderPath + data[\"filename\"], index_col=None, header=0\n",
    "                )\n",
    "                dataFrame = self.ConvertDaytoDateTime(dataFrame)\n",
    "                dataFrame = self.Get1HrIntervals(dataFrame, \"day\")\n",
    "                dataFrame = self.DropColumns(\n",
    "                    dataFrame, renameColumns=data[\"renameColumns\"]\n",
    "                )\n",
    "                dataFrame[\"Zipcode\"] = data[\"Zipcode\"]\n",
    "                DFColumns = list(dataFrame.columns)\n",
    "                columnsRearrange = [DFColumns[0], DFColumns[-1]] + DFColumns[1:-1]\n",
    "                dataFrame = dataFrame[columnsRearrange]\n",
    "                if type(bicycleDataFrame) == type(None):\n",
    "                    bicycleDataFrame = dataFrame\n",
    "                else:\n",
    "                    bicycleDataFrame = pd.merge(\n",
    "                        bicycleDataFrame, dataFrame, on=[\"day\", \"Zipcode\"], how=\"outer\"\n",
    "                    )\n",
    "\n",
    "        bicycleDataFrame = bicycleDataFrame.dropna()\n",
    "        bicycleDataFrame[\"day\"] = pd.to_datetime(bicycleDataFrame[\"day\"])\n",
    "        # bicycleDataFrame[\"EastBound\"] = 0\n",
    "        bicycleDataFrame[\"BestDirections\"] = bicycleDataFrame[\n",
    "            [\"NorthBound\", \"SouthBound\", \"WestBound\"]\n",
    "        ].apply(self.FindBestDirections, axis=1)\n",
    "        return bicycleDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycleDatasetFolderPath = \"Dataset/Bicycle Dataset/\"\n",
    "bicycleMetaDataFilepath = \"Dataset/Bicycle Dataset/metadata/metadata.json\"\n",
    "bicycleData = BicycleDataset(bicycleDatasetFolderPath, bicycleMetaDataFilepath)\n",
    "bicycleDataFrame = bicycleData.bicycleDataFrame\n",
    "print(\"bicycleDataFrame Shape\", bicycleDataFrame.shape)\n",
    "bicycleDataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherDataset:\n",
    "    def __init__(self, weatherDatasetFolderPath):\n",
    "        self.folderPath = weatherDatasetFolderPath\n",
    "        self.replaceDirection = {\n",
    "            \"ESE\": \"E\",\n",
    "            \"SSE\": \"S\",\n",
    "            \"WSW\": \"W\",\n",
    "            \"NNE\": \"N\",\n",
    "            \"ENE\": \"E\",\n",
    "            \"NNE\": \"N\",\n",
    "            \"SSW\": \"S\",\n",
    "            \"WNW\": \"W\",\n",
    "            \"NNW\": \"N\",\n",
    "        }\n",
    "        self.windDirectionEncoder = None\n",
    "        self.climateEncoder = None\n",
    "        self.weatherDataFrame = self.LoadDataSet()\n",
    "\n",
    "    def LoadDataSet(self):\n",
    "        weatherDataFrame = []\n",
    "        fileList = os.listdir(self.folderPath)\n",
    "        for fileName in tqdm(fileList):\n",
    "            with open(self.folderPath + fileName, \"r\") as jsonFile:\n",
    "                fileData = json.load(jsonFile)\n",
    "            for date, weather in fileData.items():\n",
    "                fileData = {}\n",
    "                fileData[\"day\"] = date\n",
    "                for key, value in weather.items():\n",
    "                    fileData[key] = value\n",
    "                weatherDataFrame.append(fileData)\n",
    "\n",
    "        weatherDataFrame = pd.DataFrame(weatherDataFrame)\n",
    "        weatherDataFrame[\"Zipcode\"] = 80309\n",
    "        weatherDataFrame[\"day\"] = pd.to_datetime(weatherDataFrame[\"day\"])\n",
    "        weatherDataFrame = self.PreprocessDataset(weatherDataFrame)\n",
    "        return weatherDataFrame\n",
    "\n",
    "    def PreprocessDataset(self, weatherDataFrame: pd.DataFrame):\n",
    "        weatherDataFrame.replace(self.replaceDirection, inplace=True)\n",
    "        windDirectionColumns = [\n",
    "            columnName\n",
    "            for columnName in weatherDataFrame.columns\n",
    "            if columnName.__contains__(\"windDir\")\n",
    "        ]\n",
    "\n",
    "        if self.windDirectionEncoder == None:\n",
    "            self.windDirectionEncoder = LabelEncoder()\n",
    "            uniquewindDirectionValues = []\n",
    "            for column in windDirectionColumns:\n",
    "                uniquewindDirectionValues += list(weatherDataFrame[column].unique())\n",
    "            self.windDirectionEncoder = self.windDirectionEncoder.fit(\n",
    "                uniquewindDirectionValues\n",
    "            )\n",
    "\n",
    "        for column in windDirectionColumns:\n",
    "            weatherDataFrame[column] = self.windDirectionEncoder.transform(\n",
    "                weatherDataFrame[column]\n",
    "            )\n",
    "\n",
    "        climateColumns = [\n",
    "            columnName\n",
    "            for columnName in weatherDataFrame.columns\n",
    "            if columnName.__contains__(\"weather\")\n",
    "        ]\n",
    "\n",
    "        if self.climateEncoder == None:\n",
    "            self.climateEncoder = LabelEncoder()\n",
    "            uniqueClimateValues = []\n",
    "            for column in climateColumns:\n",
    "                uniqueClimateValues += list(weatherDataFrame[column].unique())\n",
    "            self.climateEncoder = self.climateEncoder.fit(uniqueClimateValues)\n",
    "\n",
    "        for column in climateColumns:\n",
    "            weatherDataFrame[column] = self.climateEncoder.transform(\n",
    "                weatherDataFrame[column]\n",
    "            )\n",
    "\n",
    "        return weatherDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherDatasetFolderPath = \"Dataset/Weather Dataset/JsonFiles/\"\n",
    "weatherData = WeatherDataset(weatherDatasetFolderPath)\n",
    "weatherDataFrame = weatherData.weatherDataFrame\n",
    "print(\"weatherDataFrame Shape\", weatherDataFrame.shape)\n",
    "weatherDataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDataFrame = pd.merge(\n",
    "    bicycleDataFrame, weatherDataFrame, on=[\"day\", \"Zipcode\"], how=\"outer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDataFrame = finalDataFrame.dropna()\n",
    "finalDataFrame = finalDataFrame.drop(columns=[\"day\", \"Zipcode\"])\n",
    "finalDataFrame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = finalDataFrame[\"BestDirections\"]\n",
    "finalDataFrame = finalDataFrame.drop(\n",
    "    columns=[\"NorthBound\", \"SouthBound\", \"WestBound\", \"BestDirections\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MlBinarizer = MultiLabelBinarizer()\n",
    "MlBinarizer = MlBinarizer.fit(y)\n",
    "y = MlBinarizer.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    finalDataFrame, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"y_test shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateXGBClassifier(\n",
    "    parameters: dict = {\"tree_method\": \"hist\", \"device\": \"cuda\", \"verbosity\": 1}\n",
    "):\n",
    "    XGBModel = XGBClassifier(**parameters)\n",
    "    return XGBModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateLGBClassifier(parameters: dict = {\"device\": \"gpu\", \"verbosity\": 1}):\n",
    "    LGBModel = LGBMClassifier(**parameters)\n",
    "    return LGBModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateCBClassifier(\n",
    "    parameters: dict = {\n",
    "        \"task_type\": \"GPU\",\n",
    "        \"devices\": \"0:1\",\n",
    "        \"verbose\": 1,\n",
    "        \"iterations\": 100,\n",
    "    }\n",
    "):\n",
    "    CBModel = CatBoostClassifier(**parameters)\n",
    "    return CBModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateLRClassifier(\n",
    "    parameters: dict = {\n",
    "        \"n_jops\": -1,\n",
    "    }\n",
    "):\n",
    "    LRModel = LogisticRegression(**parameters)\n",
    "    return LRModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inputShape,\n",
    "        numClasses,\n",
    "        epochs,\n",
    "        batchSize,\n",
    "        lossFunction,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        verbose,\n",
    "    ):\n",
    "        self.verbose = verbose\n",
    "        self.lossFunction = lossFunction\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "        self.inputShape = inputShape\n",
    "        self.numClasses = numClasses\n",
    "        self.epochs = epochs\n",
    "        self.batchSize = batchSize\n",
    "        self.classes_ = np.arange(self.numClasses)\n",
    "        self.model = self.CreateCNNModel()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = X.reshape((X.shape[0], self.inputShape[0], self.inputShape[1]))\n",
    "\n",
    "        self.model.fit(\n",
    "            X, y, epochs=self.epochs, batch_size=self.batchSize, verbose=self.verbose\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = X.reshape((X.shape[0], self.inputShape[0], self.inputShape[1]))\n",
    "\n",
    "        predictions = self.model.predict(X)\n",
    "        return (predictions > 0.5).astype(\"int32\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def CreateCNNModel(self):\n",
    "        model = Sequential()\n",
    "        model.add(\n",
    "            Conv1D(\n",
    "                filters=64,\n",
    "                kernel_size=3,\n",
    "                activation=\"relu\",\n",
    "                input_shape=self.inputShape,\n",
    "            )\n",
    "        )\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(50, activation=\"relu\"))\n",
    "        model.add(Dense(self.numClasses, activation=\"sigmoid\"))\n",
    "        model.compile(\n",
    "            loss=self.lossFunction,\n",
    "            optimizer=self.optimizer,\n",
    "            metrics=self.metrics,\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inputShape,\n",
    "        numClasses,\n",
    "        epochs,\n",
    "        batchSize,\n",
    "        lossFunction,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        verbose,\n",
    "    ):\n",
    "        self.verbose = verbose\n",
    "        self.lossFunction = lossFunction\n",
    "        self.optimizer = optimizer\n",
    "        self.metrics = metrics\n",
    "        self.inputShape = inputShape\n",
    "        self.numClasses = numClasses\n",
    "        self.epochs = epochs\n",
    "        self.batchSize = batchSize\n",
    "        self.classes_ = np.arange(self.numClasses)\n",
    "        self.model = self.CreateLSTMModel()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = X.reshape((X.shape[0], self.inputShape[0], self.inputShape[1]))\n",
    "        # y = to_categorical(y, self.numClasses)\n",
    "        self.model.fit(\n",
    "            X, y, epochs=self.epochs, batch_size=self.batchSize, verbose=self.verbose\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = X.reshape((X.shape[0], self.inputShape[0], self.inputShape[1]))\n",
    "\n",
    "        predictions = self.model.predict(X)\n",
    "        return (predictions > 0.5).astype(\"int32\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def CreateLSTMModel(self):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(50, return_sequences=True, input_shape=self.inputShape))\n",
    "        model.add(LSTM(50))\n",
    "        model.add(Dense(50, activation=\"relu\"))\n",
    "        model.add(Dense(self.numClasses, activation=\"sigmoid\"))\n",
    "        model.compile(\n",
    "            loss=self.lossFunction,\n",
    "            optimizer=self.optimizer,\n",
    "            metrics=self.metrics,\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateEnsembleCombinations(MLModelsNames: list, minimumModels: int):\n",
    "    EnsembleCombinations = []\n",
    "    for length in range(minimumModels, len(MLModelsNames) + 1):\n",
    "        for combo in combinations(MLModelsNames, length):\n",
    "            EnsembleCombinations.append(list(combo))\n",
    "\n",
    "    return EnsembleCombinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFunction = \"binary_crossentropy\"\n",
    "optimizer = \"adam\"\n",
    "metrics = [\"accuracy\"]\n",
    "inputShape = (X_train.shape[1], 1)\n",
    "numClasses = y_train.shape[1]\n",
    "print(\"inputShape:\", inputShape)\n",
    "print(\"numClasses:\", numClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBModel = MultiOutputClassifier(CreateXGBClassifier())\n",
    "LGBModel = MultiOutputClassifier(CreateLGBClassifier())\n",
    "CBModel = MultiOutputClassifier(CreateCBClassifier())\n",
    "CNNModel = CNNClassifier(\n",
    "    inputShape=inputShape,\n",
    "    numClasses=numClasses,\n",
    "    epochs=100,\n",
    "    batchSize=32,\n",
    "    lossFunction=lossFunction,\n",
    "    optimizer=optimizer,\n",
    "    metrics=metrics,\n",
    "    verbose=1,\n",
    ")\n",
    "LSTMModel = LSTMClassifier(\n",
    "    inputShape=inputShape,\n",
    "    numClasses=numClasses,\n",
    "    epochs=100,\n",
    "    batchSize=32,\n",
    "    lossFunction=lossFunction,\n",
    "    optimizer=optimizer,\n",
    "    metrics=metrics,\n",
    "    verbose=1,\n",
    ")\n",
    "MLModels = {\n",
    "    # \"XGBModel\": XGBModel,\n",
    "    # \"LGBModel\": LGBModel,\n",
    "    # \"CBModel\": CBModel,\n",
    "    \"CNNModel\": CNNModel,\n",
    "    \"LSTMModel\": LSTMModel,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnsembleCombinations = CreateEnsembleCombinations(list(MLModels.keys()), 2)\n",
    "EnsembleCombinations.reverse()\n",
    "# finalEstimator = MultiOutputClassifier(LogisticRegression(n_jobs=-1))\n",
    "finalEstimator = XGBModel\n",
    "for EnsembleCombination in EnsembleCombinations:\n",
    "    print(EnsembleCombination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for EnsembleCombination in EnsembleCombinations:\n",
    "    estimators = []\n",
    "    for modelName in EnsembleCombination:\n",
    "        estimators.append((modelName, MLModels[modelName]))\n",
    "    print(\"EnsembleClassifer combination:\", EnsembleCombination)\n",
    "    EnsembleClassifer = StackingClassifier(\n",
    "        estimators=estimators, verbose=1, final_estimator=finalEstimator\n",
    "    )\n",
    "    EnsembleClassifer.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = EnsembleClassifer.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Ensemble model accuracy: {accuracy}\")\n",
    "    c = 0\n",
    "    for ypred, yacc in zip(\n",
    "        MlBinarizer.inverse_transform(y_pred), MlBinarizer.inverse_transform(y_test)\n",
    "    ):\n",
    "        # print( ypred, yacc)\n",
    "        if any(label in yacc for label in ypred):\n",
    "            c += 1\n",
    "    customAcc = c / len(y_test)\n",
    "    print(customAcc)\n",
    "\n",
    "    pklFileName = \"-\".join(EnsembleCombination)\n",
    "    with open(pklFileName + \".pkl\", \"wb\") as pklFile:\n",
    "        pickle.dump(EnsembleClassifer, pklFile)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"MlBinarizer\" + \".pkl\", \"wb\") as pklFile:\n",
    "    pickle.dump(MlBinarizer, pklFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"climateEncoder\" + \".pkl\", \"wb\") as pklFile:\n",
    "    pickle.dump(weatherData.climateEncoder, pklFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"windDirectionEncoder\" + \".pkl\", \"wb\") as pklFile:\n",
    "    pickle.dump(weatherData.windDirectionEncoder, pklFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Params:\n",
    "# XGBModel params: {'device': 'cuda', 'tree_method': 'hist', 'verbosity': 0, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 0}\n",
    "# LGBModel params: {'device': 'gpu', 'verbosity': 0, 'num_leaves': 31, 'max_depth': -1, 'learning_rate': 0.01, 'n_estimators': 100, 'reg_lambda': 0, 'reg_alpha': 0}\n",
    "# CBModel params: {'task_type': 'GPU', 'devices': '0:1', 'verbose': 0, 'depth': 5, 'learning_rate': 0.1, 'iterations': 1000}\n",
    "# LRModel params: {'max_iter': 100, 'n_jobs': -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperParametersRanges = {\n",
    "#     \"XGB:device\": [\"cuda\"],\n",
    "#     # \"XGB:objective\": [\"multi:softmax\"],\n",
    "#     \"XGB:tree_method\": [\"hist\"],\n",
    "#     \"XGB:verbosity\": [0],\n",
    "#     \"XGB:max_depth\": [3, 4, 5],\n",
    "#     \"XGB:learning_rate\": [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "#     \"XGB:gamma\": [0, 0.1],\n",
    "#     \"LGB:device\": [\"gpu\"],\n",
    "#     \"LGB:verbosity\": [0],\n",
    "#     \"LGB:num_leaves\": [31, 50, 100, 150],\n",
    "#     \"LGB:max_depth\": [-1, 5, 15, 20],\n",
    "#     \"LGB:learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "#     \"LGB:n_estimators\": [100, 500, 1000],\n",
    "#     \"LGB:reg_lambda\": [0, 0.01, 0.1],\n",
    "#     \"LGB:reg_alpha\": [0, 0.01, 0.1],\n",
    "#     \"CB:task_type\": [\"GPU\"],\n",
    "#     \"CB:devices\": [\"0:1\"],\n",
    "#     \"CB:verbose\": [0],\n",
    "#     \"CB:depth\": [5, 10],\n",
    "#     \"CB:learning_rate\": [0.01, 0.1],\n",
    "#     \"CB:iterations\": [100, 500, 1000],\n",
    "#     # \"LR:penalty\": [\"l1\", \"l2\", \"elasticnet\", \"none\"],\n",
    "#     # \"LR:C\": [0.001, 0.01, 0.1, 1],\n",
    "#     # \"LR:solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n",
    "#     \"LR:max_iter\": [100, 500],\n",
    "#     # \"LR:l1_ratio\": [0, 0.5, 1],\n",
    "#     \"LR:n_jobs\": [-1],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramGrids = [\n",
    "#     dict(zip(hyperParametersRanges.keys(), values))\n",
    "#     for values in product(*hyperParametersRanges.values())\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBParams = {key.split(':')[1]: value for key, value in paramGrid.items() if key.startswith('XGB:')}\n",
    "# LGBParams = {key.split(':')[1]: value for key, value in paramGrid.items() if key.startswith('LGB:')}\n",
    "# CBParams = {key.split(':')[1]: value for key, value in paramGrid.items() if key.startswith('CB:')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for paramGrid in paramGrids:\n",
    "#     XGBParams = {\n",
    "#         key.split(\":\")[1]: value\n",
    "#         for key, value in paramGrid.items()\n",
    "#         if key.startswith(\"XGB:\")\n",
    "#     }\n",
    "#     LGBParams = {\n",
    "#         key.split(\":\")[1]: value\n",
    "#         for key, value in paramGrid.items()\n",
    "#         if key.startswith(\"LGB:\")\n",
    "#     }\n",
    "#     CBParams = {\n",
    "#         key.split(\":\")[1]: value\n",
    "#         for key, value in paramGrid.items()\n",
    "#         if key.startswith(\"CB:\")\n",
    "#     }\n",
    "#     LRParams = {\n",
    "#         key.split(\":\")[1]: value\n",
    "#         for key, value in paramGrid.items()\n",
    "#         if key.startswith(\"LR:\")\n",
    "#     }\n",
    "#     XGBModel = MultiOutputClassifier(CreateXGBClassifier(XGBParams))\n",
    "#     LGBModel = MultiOutputClassifier(CreateLGBClassifier(LGBParams))\n",
    "#     CBModel = MultiOutputClassifier(CreateCBClassifier(CBParams))\n",
    "#     LRModel = MultiOutputClassifier(CreateLRClassifier(LRParams))\n",
    "#     MLModels = {\n",
    "#         \"XGBModel\": XGBModel,\n",
    "#         \"LGBModel\": LGBModel,\n",
    "#         \"CBModel\": CBModel,\n",
    "#     }\n",
    "#     # EnsembleCombinations = CreateEnsembleCombinations(list(MLModels.keys()), 2)\n",
    "#     # for EnsembleCombination in EnsembleCombinations:\n",
    "#     # print(EnsembleCombination)\n",
    "#     # for EnsembleCombination in EnsembleCombinations:\n",
    "#     print(\"-\"*30)\n",
    "#     print(\"XGBModel params:\", XGBParams)\n",
    "#     print(\"LGBModel params:\", LGBParams)\n",
    "#     print(\"CBModel params:\", CBParams)\n",
    "#     print(\"LRModel params:\", LRParams)\n",
    "#     estimators = []\n",
    "\n",
    "#     for key in MLModels.keys():\n",
    "#         estimators.append((key, MLModels[key]))\n",
    "\n",
    "#     EnsembleClassifer = StackingClassifier(\n",
    "#         estimators=estimators, verbose=1, final_estimator=LRModel\n",
    "#     )\n",
    "#     EnsembleClassifer.fit(X_train, y_train)\n",
    "\n",
    "#     y_pred = EnsembleClassifer.predict(X_test)\n",
    "\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     print(f\"Ensemble model accuracy: {accuracy}\")\n",
    "#     c = 0\n",
    "#     for ypred, yacc in zip(\n",
    "#         MlBinarizer.inverse_transform(y_pred), MlBinarizer.inverse_transform(y_test)\n",
    "#     ):\n",
    "#         if any(label in yacc for label in ypred):\n",
    "#             c += 1\n",
    "#     print(f\"Ensemble model custom accuracy: {c / len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# emailConfig = {\n",
    "#     \"sendersEmailId\": \"99kalitkar@gmail.com\",\n",
    "#     \"sendersMessage\": \"BigData Project executed\",\n",
    "#     \"sendersSubject\": f\"Accuracy: {accuracy}, {customAcc}\",\n",
    "# }\n",
    "# response = requests.post(\n",
    "#     url=\"https://www.restapi.99kalitkar.in/email\",\n",
    "#     json=emailConfig,\n",
    "#     headers={\"Content-Type\": \"application/json\"},\n",
    "# )\n",
    "# print(response.json())\n",
    "# if response.json().get(\"success\", False):\n",
    "#     print(\"Thanks for your Email. I will respond as soon as possible!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
